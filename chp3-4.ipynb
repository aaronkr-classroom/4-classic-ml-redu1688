{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "817d8c5e",
   "metadata": {},
   "source": [
    "# Chp 3 Introduction to ML\n",
    "\n",
    "Decision tree construction relies on recursion, where a function calls itself on smaller versions of the problem until reaching a stopping condition. For example, the factorial function can be defined recursively:\n",
    "\n",
    "```python\n",
    "ùëõ! = ùëõ √ó (ùëõ‚àí1)!\n",
    "```\n",
    "\n",
    "In the same way, decision trees build subtrees by applying the same process recursively on subsets of the training data, until reaching a leaf node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b27e415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current value of n: 5\n",
      "Current value of n: 4\n",
      "Current value of n: 3\n",
      "Current value of n: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Factorial\n",
    "def factorial(n):\n",
    "    if (n<=1):\n",
    "        return 1\n",
    "    else:\n",
    "        print(\"Current value of n:\",n)\n",
    "        return n*factorial (n-1)\n",
    "factorial(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2602f069",
   "metadata": {},
   "source": [
    "# Chp 4 Experiments with Classical Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6703acb",
   "metadata": {},
   "source": [
    "## Iris\n",
    "\n",
    "The iris dataset contains four continuous features‚Äîsepal length, sepal width, petal length, and petal width‚Äîand three classes corresponding to iris species. It has 150 samples, 50 per class. Using PCA augmentation, we expand the dataset to 1,200 training samples while keeping the same test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81c923a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest centroid:\n",
      "\tpredictions: [1 0 1 1 0 1 2 2 0 1 2 2 0 2 0 2 2 2 1 2 1 1 1 2 0 1 1 0 1 2]\n",
      "\tactual labels: [1 0 1 1 0 1 2 2 0 1 2 2 0 2 0 1 2 2 1 2 1 1 2 2 0 1 2 0 1 2]\n",
      "\taccuracy score: 0.9000\n",
      "\n",
      "k-NN classifier (k=3):\n",
      "\tpredictions: [1 0 1 1 0 1 2 2 0 1 2 2 0 2 0 1 2 2 1 2 1 1 2 2 0 1 1 0 1 2]\n",
      "\tactual labels: [1 0 1 1 0 1 2 2 0 1 2 2 0 2 0 1 2 2 1 2 1 1 2 2 0 1 2 0 1 2]\n",
      "\taccuracy score: 0.9667\n",
      "\n",
      "Naive Bayes classifier (Gaussian):\n",
      "\tpredictions: [1 0 1 1 0 1 2 2 0 1 2 2 0 2 0 2 2 2 1 2 1 1 2 2 0 1 1 0 1 2]\n",
      "\tactual labels: [1 0 1 1 0 1 2 2 0 1 2 2 0 2 0 1 2 2 1 2 1 1 2 2 0 1 2 0 1 2]\n",
      "\taccuracy score: 0.9333\n",
      "\n",
      "Naive Bayes classifier (Multinomial):\n",
      "\tpredictions: [1 0 1 1 0 1 2 2 0 1 2 1 0 2 0 1 2 2 1 2 1 1 2 2 0 1 2 0 1 2]\n",
      "\tactual labels: [1 0 1 1 0 1 2 2 0 1 2 2 0 2 0 1 2 2 1 2 1 1 2 2 0 1 2 0 1 2]\n",
      "\taccuracy score: 0.9667\n",
      "\n",
      "Decision tree classifier:\n",
      "\tpredictions: [1 0 1 1 0 1 2 2 0 1 2 1 0 2 0 1 2 2 1 2 1 1 2 2 0 1 1 0 1 2]\n",
      "\tactual labels: [1 0 1 1 0 1 2 2 0 1 2 2 0 2 0 1 2 2 1 2 1 1 2 2 0 1 2 0 1 2]\n",
      "\taccuracy score: 0.9333\n",
      "\n",
      "Random forest classifier (estimators=5):\n",
      "\tpredictions: [1 0 1 1 0 1 2 2 0 1 2 2 0 2 0 1 2 2 1 2 1 1 2 2 0 1 1 0 1 2]\n",
      "\tactual labels: [1 0 1 1 0 1 2 2 0 1 2 2 0 2 0 1 2 2 1 2 1 1 2 2 0 1 2 0 1 2]\n",
      "\taccuracy score: 0.9667\n",
      "\n",
      "SVM (linear, C=1.0):\n",
      "\tpredictions: [1 0 1 1 0 1 2 2 0 1 2 2 0 2 0 1 2 2 1 2 1 1 2 2 0 1 2 0 1 2]\n",
      "\tactual labels: [1 0 1 1 0 1 2 2 0 1 2 2 0 2 0 1 2 2 1 2 1 1 2 2 0 1 2 0 1 2]\n",
      "\taccuracy score: 1.0000\n",
      "\n",
      "SVM (RBF, C=1.0, gamma=0.25):\n",
      "\tpredictions: [1 0 1 1 0 1 2 2 0 1 2 2 0 2 0 1 2 2 1 2 1 1 2 2 0 1 1 0 1 2]\n",
      "\tactual labels: [1 0 1 1 0 1 2 2 0 1 2 2 0 2 0 1 2 2 1 2 1 1 2 2 0 1 2 0 1 2]\n",
      "\taccuracy score: 0.9667\n",
      "\n",
      "SVM (RBF, C=1.0, gamma=0.001, augmented):\n",
      "\tpredictions: [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\tactual labels: [1 0 1 1 0 1 2 2 0 1 2 2 0 2 0 1 2 2 1 2 1 1 2 2 0 1 2 0 1 2]\n",
      "\taccuracy score: 0.2333\n",
      "\n",
      "SVM (RBF, C=1.0, gamma=0.001, original):\n",
      "\tpredictions: [0 0 0 0 0 0 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 0 1]\n",
      "\tactual labels: [1 0 1 1 0 1 2 2 0 1 2 2 0 2 0 1 2 2 1 2 1 1 2 2 0 1 2 0 1 2]\n",
      "\taccuracy score: 0.2667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iris Experiments\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestCentroid, KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "def load_iris_data():\n",
    "    \"\"\"Load and prepare the iris dataset\"\"\"\n",
    "    iris = load_iris()\n",
    "    x = iris.data  # shape (150,4)\n",
    "    y = iris.target\n",
    "\n",
    "    # Shuffle to match train/test splits\n",
    "    np.random.seed(42)  # for reproducibility\n",
    "    indices = np.random.permutation(len(x))\n",
    "    x = x[indices]\n",
    "    y = y[indices]\n",
    "    return x, y\n",
    "\n",
    "def create_augmented_data(x_train, y_train, x_test, y_test):\n",
    "    \"\"\"Create augmented data by adding noise and scaling\"\"\"\n",
    "    # Add noise to training data\n",
    "    np.random.seed(42)\n",
    "    noise = np.random.normal(0, 0.1, x_train.shape)  # Gaussian noise\n",
    "    xa_train = x_train + noise\n",
    "    ya_train = y_train.copy()\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    xa_train = scaler.fit_transform(xa_train)\n",
    "    xa_test = scaler.transform(x_test)\n",
    "\n",
    "    return xa_train, ya_train, xa_test, y_test\n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf):\n",
    "    clf.fit(x_train, y_train)  # Fixed\n",
    "    predictions = clf.predict(x_test)\n",
    "    acc = clf.score(x_test, y_test)\n",
    "    print(\"\\tpredictions:\", predictions)\n",
    "    print(\"\\tactual labels:\", y_test)\n",
    "    print(f\"\\taccuracy score: {acc:.4f}\")\n",
    "    print()\n",
    "\n",
    "def main():\n",
    "    # Load iris data\n",
    "    x, y = load_iris_data()\n",
    "\n",
    "    # Split data (120 for training, 30 for testing)\n",
    "    N = 120\n",
    "    x_train, x_test = x[:N], x[N:]\n",
    "    y_train, y_test = y[:N], y[N:]\n",
    "\n",
    "    # Create augmented data\n",
    "    xa_train, ya_train, xa_test, ya_test = create_augmented_data(x_train, y_train, x_test, y_test)\n",
    "\n",
    "    print(\"Nearest centroid:\")\n",
    "    run(x_train, y_train, x_test, y_test, NearestCentroid())\n",
    "\n",
    "    print(\"k-NN classifier (k=3):\")\n",
    "    run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=3))\n",
    "\n",
    "    print(\"Naive Bayes classifier (Gaussian):\")\n",
    "    run(x_train, y_train, x_test, y_test, GaussianNB())\n",
    "\n",
    "    print(\"Naive Bayes classifier (Multinomial):\")\n",
    "    run(x_train, y_train, x_test, y_test, MultinomialNB())\n",
    "\n",
    "    print(\"Decision tree classifier:\")\n",
    "    run(x_train, y_train, x_test, y_test, DecisionTreeClassifier())\n",
    "\n",
    "    print(\"Random forest classifier (estimators=5):\")\n",
    "    run(xa_train, ya_train, xa_test, ya_test, RandomForestClassifier(n_estimators=5))\n",
    "\n",
    "    print(\"SVM (linear, C=1.0):\")\n",
    "    run(xa_train, ya_train, xa_test, ya_test, SVC(kernel=\"linear\", C=1.0))\n",
    "\n",
    "    print(\"SVM (RBF, C=1.0, gamma=0.25):\")\n",
    "    run(xa_train, ya_train, xa_test, ya_test, SVC(kernel=\"rbf\", C=1.0, gamma=0.25))\n",
    "\n",
    "    print(\"SVM (RBF, C=1.0, gamma=0.001, augmented):\")\n",
    "    run(xa_train, ya_train, xa_test, ya_test, SVC(kernel=\"rbf\", C=1.0, gamma=0.001))\n",
    "\n",
    "    print(\"SVM (RBF, C=1.0, gamma=0.001, original):\")\n",
    "    run(x_train, y_train, x_test, y_test, SVC(kernel=\"rbf\", C=1.0, gamma=0.001))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c231fbb",
   "metadata": {},
   "source": [
    "### Implementing a Nearest-Centroid Classifier\n",
    "\n",
    "Even without sklearn, we can quickly implement a nearest-centroid classifier for the iris dataset. The process involves calculating the per-feature means (centroids) of each class from the training samples. This is all that is needed to \"train\" the model. Predictions are made by computing the Euclidean distance from each test sample to the three centroids, assigning the sample to the class with the nearest centroid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c1b7910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: [1 0 1 1 0 1 2 2 0 1 2 2 0 2 0 2 2 2 1 2 1 1 1 2 0 1 1 0 1 2]\n",
      "actual   : [1 0 1 1 0 1 2 2 0 1 2 2 0 2 0 1 2 2 1 2 1 1 2 2 0 1 2 0 1 2]\n",
      "test accuracy = 0.9000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "def load_iris_data():\n",
    "    \"\"\"Load and prepare the iris dataset\"\"\"\n",
    "    iris = load_iris()\n",
    "    x = iris.data\n",
    "    y = iris.target\n",
    "    \n",
    "    # Shuffle the data to match typical train/test splits\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    indices = np.random.permutation(len(x))\n",
    "    x = x[indices]\n",
    "    y = y[indices]\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "def centroids(x, y):\n",
    "    \"\"\"Compute centroids for each class\"\"\"\n",
    "    c0 = x[y == 0].mean(axis=0)\n",
    "    c1 = x[y == 1].mean(axis=0)\n",
    "    c2 = x[y == 2].mean(axis=0)\n",
    "    return [c0, c1, c2]\n",
    "\n",
    "def predict(c0, c1, c2, x):\n",
    "    \"\"\"Predict class by nearest centroid\"\"\"\n",
    "    p = np.zeros(x.shape[0], dtype=\"uint8\")\n",
    "    for i in range(x.shape[0]):\n",
    "        d = [\n",
    "            ((c0 - x[i])**2).sum(),  # distance to c0\n",
    "            ((c1 - x[i])**2).sum(),  # distance to c1\n",
    "            ((c2 - x[i])**2).sum()   # distance to c2\n",
    "        ]\n",
    "        p[i] = np.argmin(d)  # index of nearest centroid\n",
    "    return p\n",
    "\n",
    "def main():\n",
    "    # Load iris data using sklearn\n",
    "    x, y = load_iris_data()\n",
    "    \n",
    "    # Split data (120 for training, rest for testing)\n",
    "    N = 120  # total data = 150\n",
    "    x_train, x_test = x[:N], x[N:]\n",
    "    y_train, y_test = y[:N], y[N:]\n",
    "    \n",
    "    # Calculate centroids and make predictions\n",
    "    c0, c1, c2 = centroids(x_train, y_train)\n",
    "    p = predict(c0, c1, c2, x_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    nc = np.sum(p == y_test)\n",
    "    nw = np.sum(p != y_test)\n",
    "    acc = nc / (nc + nw)\n",
    "    \n",
    "    print(\"predicted:\", p)\n",
    "    print(\"actual   :\", y_test)\n",
    "    print(\"test accuracy = %0.4f\" % acc)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2059011e",
   "metadata": {},
   "source": [
    "## Breast Cancer\n",
    "\n",
    "The breast cancer dataset contains 569 samples, each with 30 continuous features, including 212 malignant and 357 benign cases. Before training, we normalize the dataset by subtracting the mean and dividing by the standard deviation for each feature. Normalization ensures all features are on a similar scale, improving performance for many models.\n",
    "\n",
    "Using an 80/20 train-test split (455 training samples and 114 test samples), we train nine classifiers: nearest centroid, k-NN, naive Bayes, decision tree, random forest (two variants), linear SVM, and RBF SVM. For the SVMs, we set the margin constant C to the default 1.0, and Œ≥ for the RBF kernel to 0.0333 (1/30).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33e6bda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset info: 569 samples, 30 features\n",
      "Training set: 455 samples\n",
      "Test set: 114 samples\n",
      "Class distribution - Training: [165 290], Test: [47 67]\n",
      "\n",
      "Nearest centroid:\n",
      "\tscore = 0.9298\n",
      "\n",
      "k-NN classifier (k=3):\n",
      "\tscore = 0.9474\n",
      "\n",
      "k-NN classifier (k=7):\n",
      "\tscore = 0.9386\n",
      "\n",
      "Naive Bayes classifier (Gaussian):\n",
      "\tscore = 0.9298\n",
      "\n",
      "Decision tree classifier:\n",
      "\tscore = 0.9211\n",
      "\n",
      "Random forest classifier (estimators=5):\n",
      "\tscore = 0.9298\n",
      "\n",
      "Random forest classifier (estimators=50):\n",
      "\tscore = 0.9561\n",
      "\n",
      "SVM (linear, C=1.0):\n",
      "\tscore = 0.9649\n",
      "\n",
      "SVM (RBF, C=1.0, gamma=0.03333):\n",
      "\tscore = 0.9737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BC experiments\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestCentroid, KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def load_breast_cancer_data():\n",
    "    \"\"\"Load and prepare the breast cancer dataset with standardization\"\"\"\n",
    "    cancer = load_breast_cancer()\n",
    "    x = cancer.data\n",
    "    y = cancer.target\n",
    "    \n",
    "    # Shuffle the data for better training\n",
    "    np.random.seed(42)  # for reproducibility\n",
    "    indices = np.random.permutation(len(x))\n",
    "    x = x[indices]\n",
    "    y = y[indices]\n",
    "\n",
    "    # Normalize the features (standardization: mean=0, std=1)\n",
    "    scaler = StandardScaler() \n",
    "    x_standardized = scaler.fit_transform(x)\n",
    "\n",
    "    return x_standardized, y\n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf):\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(\"\\tscore = %0.4f\" % clf.score(x_test, y_test))\n",
    "    print()\n",
    "\n",
    "def main():\n",
    "    # Load breast cancer data with standardization\n",
    "    x, y = load_breast_cancer_data()\n",
    "    \n",
    "    # Split data (455 for training, rest for testing)\n",
    "    N = 455 \n",
    "    x_train, x_test = x[:N], x[N:]\n",
    "    y_train, y_test = y[:N], y[N:]\n",
    "    \n",
    "    print(f\"Dataset info: {len(x)} samples, {x.shape[1]} features\")\n",
    "    print(f\"Training set: {len(x_train)} samples\")\n",
    "    print(f\"Test set: {len(x_test)} samples\")\n",
    "    print(f\"Class distribution - Training: {np.bincount(y_train)}, Test: {np.bincount(y_test)}\")\n",
    "    print()\n",
    "\n",
    "    print(\"Nearest centroid:\")\n",
    "    run(x_train, y_train, x_test, y_test, NearestCentroid())\n",
    "\n",
    "    print(\"k-NN classifier (k=3):\")\n",
    "    run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=3))\n",
    "\n",
    "    print(\"k-NN classifier (k=7):\")\n",
    "    run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=7))\n",
    "\n",
    "    print(\"Naive Bayes classifier (Gaussian):\")\n",
    "    run(x_train, y_train, x_test, y_test, GaussianNB())\n",
    "\n",
    "    print(\"Decision tree classifier:\")\n",
    "    run(x_train, y_train, x_test, y_test, DecisionTreeClassifier())\n",
    "\n",
    "    print(\"Random forest classifier (estimators=5):\")\n",
    "    run(x_train, y_train, x_test, y_test, RandomForestClassifier(n_estimators=5))\n",
    "\n",
    "    print(\"Random forest classifier (estimators=50):\")\n",
    "    run(x_train, y_train, x_test, y_test, RandomForestClassifier(n_estimators=50))\n",
    "\n",
    "    print(\"SVM (linear, C=1.0):\")\n",
    "    run(x_train, y_train, x_test, y_test, SVC(kernel=\"linear\", C=1.0))\n",
    "\n",
    "    print(\"SVM (RBF, C=1.0, gamma=0.03333):\")\n",
    "    run(x_train, y_train, x_test, y_test, SVC(kernel=\"rbf\", C=1.0, gamma=0.03333))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86ee989",
   "metadata": {},
   "source": [
    "### Adding k-Fold Validation\n",
    "\n",
    "To implement k-fold validation, we first select a value for k. For the breast cancer dataset with 569 samples, a balance is needed: smaller k ensures each fold has enough samples to represent the data reasonably, while larger k helps average out the effects of a ‚Äúbad‚Äù split. A common choice is k = 5, giving roughly 113 samples per fold, with 80% for training and 20% for testing. The code is designed to allow easy adjustment of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0e312fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid folds argument. Using 5.\n",
      "Performing 5-fold cross-validation on breast cancer dataset\n",
      "Dataset: 569 samples, 30 features\n",
      "\n",
      "Results (mean +/- stderr | individual fold scores):\n",
      "----------------------------------------------------------------------\n",
      "Nearest centroid    : 0.9315 +/- 0.0076 | 0.9561 0.9035 0.9298 0.9386 0.9292 \n",
      "3-NN                : 0.9613 +/- 0.0064 | 0.9474 0.9737 0.9561 0.9825 0.9469 \n",
      "7-NN                : 0.9613 +/- 0.0073 | 0.9474 0.9825 0.9737 0.9649 0.9381 \n",
      "Naive Bayes         : 0.9367 +/- 0.0068 | 0.9649 0.9211 0.9386 0.9298 0.9292 \n",
      "Decision tree       : 0.9280 +/- 0.0067 | 0.9298 0.9211 0.9035 0.9386 0.9469 \n",
      "Random forest (5)   : 0.9508 +/- 0.0040 | 0.9386 0.9649 0.9474 0.9474 0.9558 \n",
      "Random forest (50)  : 0.9666 +/- 0.0016 | 0.9649 0.9737 0.9649 0.9649 0.9646 \n",
      "SVM (linear)        : 0.9719 +/- 0.0058 | 0.9561 0.9825 0.9649 0.9912 0.9646 \n"
     ]
    }
   ],
   "source": [
    "# bc_kfold_fixed.py\n",
    "import numpy as np\n",
    "import sys\n",
    "import traceback\n",
    "from sklearn.neighbors import NearestCentroid, KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def load_breast_cancer_data():\n",
    "    cancer = load_breast_cancer()\n",
    "    x = cancer.data\n",
    "    y = cancer.target\n",
    "    scaler = StandardScaler()\n",
    "    x_standardized = scaler.fit_transform(x)\n",
    "    return x_standardized, y\n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf):\n",
    "    clf.fit(x_train, y_train)\n",
    "    return clf.score(x_test, y_test)\n",
    "\n",
    "def split(x, y, k, m):\n",
    "    \"\"\"Return x_train, y_train, x_test, y_test for k-th fold.\n",
    "       Uses np.array_split so fold sizes can differ by 1 when n % m != 0.\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    idx_folds = np.array_split(np.arange(n), m)  # list of index arrays\n",
    "    test_idx = idx_folds[k]\n",
    "    train_idx = np.hstack([idx_folds[i] for i in range(m) if i != k])\n",
    "    return x[train_idx], y[train_idx], x[test_idx], y[test_idx]\n",
    "\n",
    "def pp(z, k, name):\n",
    "    m = z.shape[1]\n",
    "    mean = z[k].mean()\n",
    "    stderr = z[k].std() / np.sqrt(m)\n",
    "    print(\"{:20s}: {:0.4f} +/- {:0.4f} | \".format(name, mean, stderr), end='')\n",
    "    for i in range(m):\n",
    "        print(\"{:0.4f} \".format(z[k, i]), end='')\n",
    "    print()\n",
    "\n",
    "def main(argv=None):\n",
    "    try:\n",
    "        if argv is None:\n",
    "            argv = sys.argv\n",
    "\n",
    "        # Parse folds argument if provided, otherwise default to 5\n",
    "        if len(argv) >= 2:\n",
    "            try:\n",
    "                m = int(argv[1])\n",
    "                if m < 2:\n",
    "                    print(\"Number of folds must be >= 2. Using 5.\")\n",
    "                    m = 5\n",
    "            except ValueError:\n",
    "                print(\"Invalid folds argument. Using 5.\")\n",
    "                m = 5\n",
    "        else:\n",
    "            m = 5\n",
    "\n",
    "        x, y = load_breast_cancer_data()\n",
    "\n",
    "        if m > x.shape[0]:\n",
    "            print(f\"Requested {m} folds but dataset has only {x.shape[0]} samples. Reducing folds to {x.shape[0]}.\")\n",
    "            m = x.shape[0]\n",
    "\n",
    "        # Shuffle\n",
    "        np.random.seed(42)\n",
    "        perm = np.random.permutation(len(y))\n",
    "        x = x[perm]\n",
    "        y = y[perm]\n",
    "\n",
    "        print(f\"Performing {m}-fold cross-validation on breast cancer dataset\")\n",
    "        print(f\"Dataset: {x.shape[0]} samples, {x.shape[1]} features\")\n",
    "        print()\n",
    "\n",
    "        classifiers = [\n",
    "            (\"Nearest centroid\", NearestCentroid()),\n",
    "            (\"3-NN\", KNeighborsClassifier(n_neighbors=3)),\n",
    "            (\"7-NN\", KNeighborsClassifier(n_neighbors=7)),\n",
    "            (\"Naive Bayes\", GaussianNB()),\n",
    "            (\"Decision tree\", DecisionTreeClassifier()),\n",
    "            (\"Random forest (5)\", RandomForestClassifier(n_estimators=5)),\n",
    "            (\"Random forest (50)\", RandomForestClassifier(n_estimators=50)),\n",
    "            (\"SVM (linear)\", SVC(kernel=\"linear\", C=1.0))\n",
    "        ]\n",
    "\n",
    "        z = np.zeros((len(classifiers), m))\n",
    "\n",
    "        for k in range(m):\n",
    "            x_train, y_train, x_test, y_test = split(x, y, k, m)\n",
    "            for i, (_, clf) in enumerate(classifiers):\n",
    "                z[i, k] = run(x_train, y_train, x_test, y_test, clf)\n",
    "\n",
    "        print(\"Results (mean +/- stderr | individual fold scores):\")\n",
    "        print(\"-\" * 70)\n",
    "        for i, (name, _) in enumerate(classifiers):\n",
    "            pp(z, i, name)\n",
    "\n",
    "    except Exception:\n",
    "        print(\"An exception occurred ‚Äî full traceback below:\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0f8156",
   "metadata": {},
   "source": [
    "### Fine-Tuning the RBF Kernel SVM\n",
    "\n",
    "For the RBF (Gaussian) kernel SVM, both C and Œ≥ must be optimized. A 2D grid search is performed:\n",
    "\n",
    "- C uses the same range as the linear SVM.\n",
    "- Œ≥ is selected from powers of two times the default 1/30, for p ‚àà [‚Äì4, 3].\n",
    "\n",
    "For each pair (C, Œ≥), five-fold validation is performed, and the pair with the highest mean accuracy is selected. Repeated runs produce slightly different results due to randomization in the dataset ordering.\n",
    "\n",
    "One promising combination is (C, Œ≥) = (10, 0.00417), which achieves a grand mean accuracy of 97.70%, the highest among all models tested on the breast cancer dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47df69f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBF SVM Hyperparameter Search using 5-fold cross-validation\n",
      "Dataset: 569 samples, 30 features\n",
      "\n",
      "C values: [1.e-02 1.e-01 1.e+00 2.e+00 1.e+01 5.e+01 1.e+02]\n",
      "Gamma values: [0.00208333 0.00416667 0.00833333 0.01666667 0.03333333 0.06666667\n",
      " 0.13333333 0.26666667]\n",
      "\n",
      "Searching hyperparameters...\n",
      "C=  0.01, gamma= 0.00208: 0.6248 (+/- 0.0527)\n",
      "C=  0.01, gamma= 0.00417: 0.6248 (+/- 0.0527)\n",
      "C=  0.01, gamma= 0.00833: 0.6248 (+/- 0.0527)\n",
      "C=  0.01, gamma= 0.01667: 0.6301 (+/- 0.0569)\n",
      "C=  0.01, gamma= 0.03333: 0.6248 (+/- 0.0527)\n",
      "C=  0.01, gamma= 0.06667: 0.6248 (+/- 0.0527)\n",
      "C=  0.01, gamma= 0.13333: 0.6248 (+/- 0.0527)\n",
      "C=  0.01, gamma= 0.26667: 0.6248 (+/- 0.0527)\n",
      "C=  0.10, gamma= 0.00208: 0.8938 (+/- 0.0280)\n",
      "C=  0.10, gamma= 0.00417: 0.9327 (+/- 0.0132)\n",
      "C=  0.10, gamma= 0.00833: 0.9434 (+/- 0.0144)\n",
      "C=  0.10, gamma= 0.01667: 0.9451 (+/- 0.0130)\n",
      "C=  0.10, gamma= 0.03333: 0.9434 (+/- 0.0221)\n",
      "C=  0.10, gamma= 0.06667: 0.9416 (+/- 0.0278)\n",
      "C=  0.10, gamma= 0.13333: 0.9097 (+/- 0.0505)\n",
      "C=  0.10, gamma= 0.26667: 0.6248 (+/- 0.0527)\n",
      "C=  1.00, gamma= 0.00208: 0.9558 (+/- 0.0097)\n",
      "C=  1.00, gamma= 0.00417: 0.9664 (+/- 0.0103)\n",
      "C=  1.00, gamma= 0.00833: 0.9664 (+/- 0.0117)\n",
      "C=  1.00, gamma= 0.01667: 0.9717 (+/- 0.0189)\n",
      "C=  1.00, gamma= 0.03333: 0.9735 (+/- 0.0097)\n",
      "C=  1.00, gamma= 0.06667: 0.9628 (+/- 0.0142)\n",
      "C=  1.00, gamma= 0.13333: 0.9487 (+/- 0.0066)\n",
      "C=  1.00, gamma= 0.26667: 0.9292 (+/- 0.0148)\n",
      "C=  2.00, gamma= 0.00208: 0.9664 (+/- 0.0103)\n",
      "C=  2.00, gamma= 0.00417: 0.9664 (+/- 0.0117)\n",
      "C=  2.00, gamma= 0.00833: 0.9699 (+/- 0.0120)\n",
      "C=  2.00, gamma= 0.01667: 0.9752 (+/- 0.0130)\n",
      "C=  2.00, gamma= 0.03333: 0.9752 (+/- 0.0142)\n",
      "C=  2.00, gamma= 0.06667: 0.9646 (+/- 0.0125)\n",
      "C=  2.00, gamma= 0.13333: 0.9504 (+/- 0.0043)\n",
      "C=  2.00, gamma= 0.26667: 0.9310 (+/- 0.0103)\n",
      "C= 10.00, gamma= 0.00208: 0.9735 (+/- 0.0125)\n",
      "C= 10.00, gamma= 0.00417: 0.9717 (+/- 0.0130)\n",
      "C= 10.00, gamma= 0.00833: 0.9788 (+/- 0.0144)\n",
      "C= 10.00, gamma= 0.01667: 0.9735 (+/- 0.0125)\n",
      "C= 10.00, gamma= 0.03333: 0.9681 (+/- 0.0120)\n",
      "C= 10.00, gamma= 0.06667: 0.9593 (+/- 0.0120)\n",
      "C= 10.00, gamma= 0.13333: 0.9469 (+/- 0.0079)\n",
      "C= 10.00, gamma= 0.26667: 0.9274 (+/- 0.0130)\n",
      "C= 50.00, gamma= 0.00208: 0.9770 (+/- 0.0132)\n",
      "C= 50.00, gamma= 0.00417: 0.9699 (+/- 0.0144)\n",
      "C= 50.00, gamma= 0.00833: 0.9664 (+/- 0.0103)\n",
      "C= 50.00, gamma= 0.01667: 0.9717 (+/- 0.0130)\n",
      "C= 50.00, gamma= 0.03333: 0.9611 (+/- 0.0132)\n",
      "C= 50.00, gamma= 0.06667: 0.9646 (+/- 0.0158)\n",
      "C= 50.00, gamma= 0.13333: 0.9469 (+/- 0.0079)\n",
      "C= 50.00, gamma= 0.26667: 0.9274 (+/- 0.0130)\n",
      "C=100.00, gamma= 0.00208: 0.9699 (+/- 0.0090)\n",
      "C=100.00, gamma= 0.00417: 0.9664 (+/- 0.0103)\n",
      "C=100.00, gamma= 0.00833: 0.9681 (+/- 0.0106)\n",
      "C=100.00, gamma= 0.01667: 0.9699 (+/- 0.0164)\n",
      "C=100.00, gamma= 0.03333: 0.9611 (+/- 0.0154)\n",
      "C=100.00, gamma= 0.06667: 0.9646 (+/- 0.0158)\n",
      "C=100.00, gamma= 0.13333: 0.9469 (+/- 0.0079)\n",
      "C=100.00, gamma= 0.26667: 0.9274 (+/- 0.0130)\n",
      "\n",
      "==================================================\n",
      "BEST HYPERPARAMETERS:\n",
      "best C     = 10.00000\n",
      "     gamma = 0.00833\n",
      "   accuracy= 0.97876 (+/- 0.01438)\n",
      "individual fold scores: ['0.9646', '0.9735', '0.9646', '1.0000', '0.9912']\n"
     ]
    }
   ],
   "source": [
    "# BC RBF SVM Search\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def load_breast_cancer_data():\n",
    "    \"\"\"Load and prepare the breast cancer dataset with standardization\"\"\"\n",
    "    # Load the breast cancer dataset\n",
    "    cancer = load_breast_cancer()\n",
    "    x = cancer.data\n",
    "    y = cancer.target\n",
    "    \n",
    "    # Normalize the features (subtract mean and divide by std)\n",
    "    scaler = StandardScaler()\n",
    "    x_standardized = scaler.fit_transform(x)\n",
    "    \n",
    "    return x_standardized, y\n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf):\n",
    "    clf.fit(x_train, y_train)\n",
    "    return clf.score(x_test, y_test)\n",
    "\n",
    "def split(x,y,k,m):\n",
    "    ns = int(y.shape[0]/m)\n",
    "    s = []\n",
    "    for i in range(m):\n",
    "        s.append([x[(ns*i):(ns*i+ns)], y[(ns*i):(ns*i+ns)]])\n",
    "    x_test, y_test = s[k]\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    for i in range(m):\n",
    "        if (i==k):\n",
    "            continue\n",
    "        else:\n",
    "            a,b = s[i]\n",
    "            x_train.append(a)\n",
    "            y_train.append(b)\n",
    "    x_train = np.array(x_train).reshape(((m-1)*ns,30))\n",
    "    y_train = np.array(y_train).reshape((m-1)*ns)\n",
    "    return [x_train, y_train, x_test, y_test]\n",
    "\n",
    "def main():\n",
    "    m = 5 \n",
    "    \n",
    "    # Load breast cancer data with standardization\n",
    "    x, y = load_breast_cancer_data()\n",
    "    \n",
    "    # Shuffle the data\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    idx = np.argsort(np.random.random(y.shape[0]))\n",
    "    x = x[idx]\n",
    "    y = y[idx]\n",
    "    \n",
    "    print(f\"RBF SVM Hyperparameter Search using {m}-fold cross-validation\")\n",
    "    print(f\"Dataset: {x.shape[0]} samples, {x.shape[1]} features\")\n",
    "    print()\n",
    "\n",
    "    Cs = np.array([0.01,0.1,1.0,2.0,10.0,50.0,100.0])\n",
    "    gs = (1./30)*2.0**np.array([-4,-3,-2,-1,0,1,2,3])\n",
    "    \n",
    "    print(\"C values:\", Cs)\n",
    "    print(\"Gamma values:\", gs)\n",
    "    print()\n",
    "    print(\"Searching hyperparameters...\")\n",
    "    \n",
    "    zmax = 0.0 \n",
    "    best_scores = []\n",
    "    \n",
    "    for i, C in enumerate(Cs): \n",
    "        for j, g in enumerate(gs): \n",
    "            z = np.zeros(m)\n",
    "            for k in range(m):\n",
    "                x_train, y_train, x_test, y_test = split(x,y,k,m)\n",
    "                z[k] = run(x_train, y_train, x_test, y_test, SVC(C=C,gamma=g,kernel=\"rbf\"))\n",
    "            \n",
    "            mean_score = z.mean()\n",
    "            if (mean_score > zmax):\n",
    "                zmax = mean_score\n",
    "                bestC = C \n",
    "                bestg = g \n",
    "                best_scores = z.copy()\n",
    "            \n",
    "            # Print progress (optional - comment out if too verbose)\n",
    "            print(f\"C={C:6.2f}, gamma={g:8.5f}: {mean_score:.4f} (+/- {z.std():.4f})\")\n",
    "    \n",
    "    print()\n",
    "    print(\"=\" * 50)\n",
    "    print(\"BEST HYPERPARAMETERS:\")\n",
    "    print(\"best C     = %0.5f\" % bestC)\n",
    "    print(\"     gamma = %0.5f\" % bestg)\n",
    "    print(\"   accuracy= %0.5f (+/- %0.5f)\" % (zmax, best_scores.std()))\n",
    "    print(\"individual fold scores:\", [f\"{score:.4f}\" for score in best_scores])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed553cb4",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "\n",
    "The final dataset examined in this chapter is the vector version of MNIST, which contains 28√ó28 grayscale images of handwritten digits (0‚Äì9), one per image. MNIST is a foundational dataset in machine learning and deep learning and will be used throughout the book.\n",
    "\n",
    "MNIST has 60,000 training images and 10,000 test images, roughly balanced across the 10 digits. Because the dataset is large, classical models are trained directly on the training set and tested on the test set, without using k-fold validation.\n",
    "\n",
    "The images are converted into vectors of 784 elements (28 √ó 28 pixels), with values from 0 to 255. Three versions of the dataset are considered:\n",
    "\n",
    "1. Raw byte values (0‚Äì255)\n",
    "2. Scaled data to [0, 1) by dividing by 256\n",
    "3. Normalized data, where each pixel has its mean subtracted and is divided by its standard deviation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df99a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: MNIST (mnist_784)\n",
      "Training samples: 60000, Test samples: 10000\n",
      "Features: 784 (28x28 images flattened)\n",
      "Classes: 10\n",
      "Classes: 0-9 (digits)\n",
      "\n",
      "Running classifier experiments...\n",
      "======================================================================\n",
      "    Nearest centroid          : 0.8203  (fit: 0.15s, predict: 0.02s)\n",
      "    k-NN classifier (k=3)     : 0.9705  (fit: 0.09s, predict: 12.41s)\n",
      "    k-NN classifier (k=7)     : 0.9694  (fit: 0.08s, predict: 12.35s)\n",
      "    Naive Bayes (Gaussian)    : 0.5558  (fit: 0.25s, predict: 0.17s)\n",
      "    Decision tree             : 0.8793  (fit: 17.65s, predict: 0.01s)\n",
      "    Random forest (trees=  5) : 0.9212  (fit: 1.87s, predict: 0.01s)\n",
      "    Random forest (trees= 50) : 0.9672  (fit: 18.29s, predict: 0.10s)\n",
      "    Random forest (trees=500) : "
     ]
    }
   ],
   "source": [
    "# MNIST experiments (fixed)\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestCentroid, KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import decomposition\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "def load_mnist_data(fashion=False):\n",
    "    \"\"\"Load MNIST digits or Fashion-MNIST (tries OpenML, then keras fallback).\n",
    "       Returns: x_train, y_train, x_test, y_test (float32 features scaled 0-1).\n",
    "    \"\"\"\n",
    "    if not fashion:\n",
    "        dataset_name = \"MNIST (mnist_784)\"\n",
    "        data = fetch_openml(\"mnist_784\", version=1, as_frame=False)\n",
    "        X = data[\"data\"].astype(np.float32)\n",
    "        y = data[\"target\"].astype(np.int64)\n",
    "    else:\n",
    "        dataset_name = \"Fashion-MNIST\"\n",
    "        try:\n",
    "            data = fetch_openml(\"Fashion-MNIST\", version=1, as_frame=False)\n",
    "            X = data[\"data\"].astype(np.float32)\n",
    "            y = data[\"target\"].astype(np.int64)\n",
    "        except Exception:\n",
    "            # fallback to keras (if installed)\n",
    "            try:\n",
    "                from tensorflow.keras.datasets import fashion_mnist\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(\n",
    "                    \"Could not fetch Fashion-MNIST from OpenML and keras is not available. \"\n",
    "                    \"Install tensorflow or ensure OpenML dataset is accessible.\"\n",
    "                ) from e\n",
    "            (xtr, ytr), (xte, yte) = fashion_mnist.load_data()\n",
    "            X = np.vstack([xtr.reshape(len(xtr), -1), xte.reshape(len(xte), -1)]).astype(np.float32)\n",
    "            y = np.concatenate([ytr, yte]).astype(np.int64)\n",
    "\n",
    "    # Normalize pixel values to [0,1]\n",
    "    # Some openml versions store values 0..255; dividing is safe either way.\n",
    "    X /= 255.0\n",
    "\n",
    "    n = X.shape[0]\n",
    "    # Standard MNIST split: 60k train / 10k test if available; otherwise use ~85/15 split\n",
    "    if (not fashion) and (n >= 70000):\n",
    "        train_size = 60000\n",
    "    else:\n",
    "        train_size = int(n * 0.857142857)  # ~60k/70k ratio\n",
    "\n",
    "    x_train = X[:train_size]\n",
    "    y_train = y[:train_size]\n",
    "    x_test = X[train_size:]\n",
    "    y_test = y[train_size:]\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    print(f\"Training samples: {len(x_train)}, Test samples: {len(x_test)}\")\n",
    "    print(f\"Features: {x_train.shape[1]} (28x28 images flattened)\" )\n",
    "    print(f\"Classes: {len(np.unique(y))}\")\n",
    "    if fashion:\n",
    "        class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                       'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "        print(f\"Class names: {class_names}\")\n",
    "    else:\n",
    "        print(\"Classes: 0-9 (digits)\")\n",
    "    print()\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf):\n",
    "    \"\"\"Fit clf, predict, print accuracy and timing. Returns accuracy.\"\"\"\n",
    "    t0 = time.time()\n",
    "    clf.fit(x_train, y_train)\n",
    "    fit_time = time.time() - t0\n",
    "\n",
    "    t1 = time.time()\n",
    "    preds = clf.predict(x_test)\n",
    "    predict_time = time.time() - t1\n",
    "\n",
    "    acc = float(np.mean(preds == y_test))\n",
    "    # print single-line result (this function is usually called after a print(..., end=''))\n",
    "    print(f\"{acc:.4f}  (fit: {fit_time:.2f}s, predict: {predict_time:.2f}s)\")\n",
    "    return acc\n",
    "\n",
    "def train(x_train, y_train, x_test, y_test):\n",
    "    print(\"    Nearest centroid          : \", end='')\n",
    "    run(x_train, y_train, x_test, y_test, NearestCentroid())\n",
    "\n",
    "    print(\"    k-NN classifier (k=3)     : \", end='')\n",
    "    run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=3))\n",
    "\n",
    "    print(\"    k-NN classifier (k=7)     : \", end='')\n",
    "    run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=7))\n",
    "\n",
    "    print(\"    Naive Bayes (Gaussian)    : \", end='')\n",
    "    run(x_train, y_train, x_test, y_test, GaussianNB())\n",
    "\n",
    "    print(\"    Decision tree             : \", end='')\n",
    "    run(x_train, y_train, x_test, y_test, DecisionTreeClassifier())\n",
    "\n",
    "    print(\"    Random forest (trees=  5) : \", end='')\n",
    "    run(x_train, y_train, x_test, y_test, RandomForestClassifier(n_estimators=5))\n",
    "\n",
    "    print(\"    Random forest (trees= 50) : \", end='')\n",
    "    run(x_train, y_train, x_test, y_test, RandomForestClassifier(n_estimators=50))\n",
    "\n",
    "    print(\"    Random forest (trees=500) : \", end='')\n",
    "    run(x_train, y_train, x_test, y_test, RandomForestClassifier(n_estimators=500))\n",
    "\n",
    "    print(\"    Random forest (trees=1000): \", end='')\n",
    "    run(x_train, y_train, x_test, y_test, RandomForestClassifier(n_estimators=1000))\n",
    "\n",
    "    print(\"    LinearSVM (C=0.01)        : \", end='')\n",
    "    run(x_train, y_train, x_test, y_test, LinearSVC(C=0.01, max_iter=2000))\n",
    "\n",
    "    print(\"    LinearSVM (C=0.1)         : \", end='')\n",
    "    run(x_train, y_train, x_test, y_test, LinearSVC(C=0.1, max_iter=2000))\n",
    "\n",
    "    print(\"    LinearSVM (C=1.0)         : \", end='')\n",
    "    run(x_train, y_train, x_test, y_test, LinearSVC(C=1.0, max_iter=2000))\n",
    "\n",
    "    print(\"    LinearSVM (C=10.0)        : \", end='')\n",
    "    run(x_train, y_train, x_test, y_test, LinearSVC(C=10.0, max_iter=2000))\n",
    "\n",
    "def main():\n",
    "    # SWITCH BETWEEN DATASETS: set fashion=True for Fashion-MNIST\n",
    "    x_train, y_train, x_test, y_test = load_mnist_data(fashion=False)  # MNIST digits\n",
    "    # x_train, y_train, x_test, y_test = load_mnist_data(fashion=True)   # Fashion-MNIST (fallback to keras if needed)\n",
    "\n",
    "    print(\"Running classifier experiments...\")\n",
    "    print(\"=\" * 70)\n",
    "    train(x_train, y_train, x_test, y_test)\n",
    "\n",
    "    print(\"\\nRunning experiments with PCA (50 components)...\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Apply PCA for dimensionality reduction\n",
    "    pca = decomposition.PCA(n_components=50)\n",
    "    x_train_pca = pca.fit_transform(x_train)\n",
    "    x_test_pca = pca.transform(x_test)\n",
    "\n",
    "    print(f\"Original features: {x_train.shape[1]}, PCA features: {x_train_pca.shape[1]}\")\n",
    "    print(f\"Explained variance ratio: {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "    print()\n",
    "\n",
    "    train(x_train_pca, y_train, x_test_pca, y_test)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b91178",
   "metadata": {},
   "source": [
    "The code uses LinearSVC instead of SVC for runtime efficiency and multiclass handling. Helper functions track both model accuracy and training/testing time, important due to the dataset‚Äôs larger size. Training is repeated for the raw, scaled, and normalized versions of the dataset.\n",
    "\n",
    "Normalization uses the training set‚Äôs mean and standard deviation, which are also applied to test data, as these better represent the true distribution. PCA is also applied, reducing the 784 features to 15 principal components, capturing just over 33% of the variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c2eddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MNIST DIGITS DATASET\n",
      "============================================================\n",
      "Models trained on raw [0,255] images:\n",
      "Data shape: (60000, 784)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import decomposition\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_mnist_data(fashion=False):\n",
    "    \"\"\"Load MNIST or Fashion-MNIST dataset\"\"\"\n",
    "    if fashion:\n",
    "        dataset = fetch_openml('Fashion-MNIST', version=1, as_frame=False, parser='auto')\n",
    "    else:\n",
    "        dataset = fetch_openml('mnist_784', version=1, as_frame=False, parser='auto')\n",
    "    \n",
    "    X, y = dataset.data, dataset.target.astype(int)\n",
    "    \n",
    "    # Split into train and test (MNIST: first 60k train, last 10k test)\n",
    "    if fashion:\n",
    "        # Fashion-MNIST is already split\n",
    "        x_train, x_test = X[:60000], X[60000:]\n",
    "        y_train, y_test = y[:60000], y[60000:]\n",
    "    else:\n",
    "        # Standard MNIST split\n",
    "        x_train, x_test = X[:60000], X[60000:]\n",
    "        y_train, y_test = y[:60000], y[60000:]\n",
    "    \n",
    "    # Normalize to [0,1] range\n",
    "    x_train = x_train.astype('float64') / 255.0\n",
    "    x_test = x_test.astype('float64') / 255.0\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def train(x_train, y_train, x_test, y_test):\n",
    "    \"\"\"Train and evaluate multiple classifiers\"\"\"\n",
    "    classifiers = {\n",
    "        'K-NN (k=3)': KNeighborsClassifier(n_neighbors=3),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'SVM (linear)': SVC(kernel='linear', random_state=42),\n",
    "        'SVM (RBF)': SVC(kernel='rbf', random_state=42),\n",
    "        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'MLP': MLPClassifier(hidden_layer_sizes=(100,), random_state=42, max_iter=1000)\n",
    "    }\n",
    "    \n",
    "    print(f\"Data shape: {x_train.shape}\")\n",
    "    \n",
    "    for name, clf in classifiers.items():\n",
    "        # Use cross-validation on training set\n",
    "        scores = cross_val_score(clf, x_train, y_train, cv=3, scoring='accuracy')\n",
    "        \n",
    "        # Also train on full training set and evaluate on test set\n",
    "        clf.fit(x_train, y_train)\n",
    "        test_score = clf.score(x_test, y_test)\n",
    "        \n",
    "        print(f\"{name:20} | CV Acc: {scores.mean():.4f} (+/- {scores.std() * 2:.4f}) | Test Acc: {test_score:.4f}\")\n",
    "    print()\n",
    "\n",
    "def main():\n",
    "    # SWITCH BETWEEN DATASETS: set fashion=True for Fashion-MNIST\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MNIST DIGITS DATASET\")\n",
    "    print(\"=\" * 60)\n",
    "    x_train, y_train, x_test, y_test = load_mnist_data(fashion=False)  # MNIST digits\n",
    "    \n",
    "    # Convert back to [0,255] range (integers but stored as float64 for consistency)\n",
    "    x_train_255 = (x_train * 255.0).astype(\"float64\")\n",
    "    x_test_255 = (x_test * 255.0).astype(\"float64\")\n",
    "\n",
    "    print(\"Models trained on raw [0,255] images:\")\n",
    "    train(x_train_255, y_train, x_test_255, y_test)\n",
    "\n",
    "    print(\"Models trained on raw [0,1] images:\")\n",
    "    train(x_train, y_train, x_test, y_test)\n",
    "\n",
    "    # Normalize by mean and std of training set\n",
    "    m = x_train_255.mean(axis=0)\n",
    "    s = x_train_255.std(axis=0) + 1e-8\n",
    "    x_ntrain = (x_train_255 - m) / s\n",
    "    x_ntest  = (x_test_255 - m) / s\n",
    "\n",
    "    print(\"Models trained on normalized images:\")\n",
    "    train(x_ntrain, y_train, x_ntest, y_test)\n",
    "\n",
    "    # PCA on normalized images (reduce to 15 features)\n",
    "    pca = decomposition.PCA(n_components=15)\n",
    "    pca.fit(x_ntrain)\n",
    "    x_ptrain = pca.transform(x_ntrain)\n",
    "    x_ptest = pca.transform(x_ntest)\n",
    "\n",
    "    print(\"Models trained on first 15 PCA components of normalized images:\")\n",
    "    train(x_ptrain, y_train, x_ptest, y_test)\n",
    "    \n",
    "    # Optional: Also run for Fashion-MNIST\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FASHION-MNIST DATASET\")\n",
    "    print(\"=\" * 60)\n",
    "    x_train_f, y_train_f, x_test_f, y_test_f = load_mnist_data(fashion=True)   # Fashion-MNIST\n",
    "    \n",
    "    x_train_f_255 = (x_train_f * 255.0).astype(\"float64\")\n",
    "    x_test_f_255 = (x_test_f * 255.0).astype(\"float64\")\n",
    "    \n",
    "    print(\"Models trained on raw [0,255] images:\")\n",
    "    train(x_train_f_255, y_train_f, x_test_f_255, y_test_f)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69bd101",
   "metadata": {},
   "source": [
    "### Experimenting with PCA Components\n",
    "\n",
    "Previously, 15 PCA components were used, representing about 33% of the dataset‚Äôs variance. To explore the effect of PCA further, the number of components is varied from 10 to 780, and three models are trained for each setting: Gaussian naive Bayes, random forest (50 trees), and linear SVM (C = 1.0). This process is computationally intensive and took over 10 hours on a low-end machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5506c2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST digits dataset...\n",
      "Dataset: MNIST digits\n",
      "Training samples: 60000, Test samples: 10000\n",
      "Features: 784 (28x28 images)\n",
      "\n",
      "Running PCA component analysis...\n",
      "Testing components from 10 to 780 (78 different values)\n",
      "\n",
      "Progress:\n",
      "Testing 10 components... (1/78) Done. (Variance explained: 0.277)\n",
      "Testing 20 components... (2/78) Done. (Variance explained: 0.381)\n",
      "Testing 30 components... (3/78) Done. (Variance explained: 0.452)\n",
      "Testing 40 components... (4/78) Done. (Variance explained: 0.506)\n",
      "Testing 50 components... (5/78) Done. (Variance explained: 0.551)\n",
      "Testing 60 components... (6/78) Done. (Variance explained: 0.589)\n",
      "Testing 70 components... (7/78) Done. (Variance explained: 0.623)\n",
      "Testing 80 components... (8/78) Done. (Variance explained: 0.653)\n",
      "Testing 90 components... (9/78) "
     ]
    }
   ],
   "source": [
    "# PCA Component Experiments\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import decomposition\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "def load_mnist_data(fashion=False):\n",
    "    \"\"\"Load MNIST digits or Fashion-MNIST dataset\"\"\"\n",
    "    if fashion:\n",
    "        # Load Fashion-MNIST dataset\n",
    "        print(\"Loading Fashion-MNIST dataset...\")\n",
    "        X, y = fetch_openml('Fashion-MNIST', version=1, return_X_y=True, as_frame=False)\n",
    "        dataset_name = \"Fashion-MNIST\"\n",
    "    else:\n",
    "        # Load MNIST digits dataset  \n",
    "        print(\"Loading MNIST digits dataset...\")\n",
    "        X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
    "        dataset_name = \"MNIST digits\"\n",
    "    \n",
    "    # Convert to proper data types\n",
    "    X = X.astype(np.float64)\n",
    "    y = y.astype(np.int32)\n",
    "    \n",
    "    # Use standard train/test split (first 60000 for train, last 10000 for test)\n",
    "    x_train = X[:60000]\n",
    "    x_test = X[60000:]\n",
    "    y_train = y[:60000] \n",
    "    y_test = y[60000:]\n",
    "    \n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    print(f\"Training samples: {len(x_train)}, Test samples: {len(x_test)}\")\n",
    "    print(f\"Features: {x_train.shape[1]} (28x28 images)\")\n",
    "    print()\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf):\n",
    "    \"\"\"Train classifier and measure time + accuracy\"\"\"\n",
    "    s = time.time()\n",
    "    clf.fit(x_train, y_train)\n",
    "    e_train = time.time() - s \n",
    "    s = time.time()\n",
    "    score = clf.score(x_test, y_test)\n",
    "    e_test = time.time() - s \n",
    "    return [score, e_train, e_test]\n",
    "\n",
    "def main():\n",
    "    # SWITCH BETWEEN DATASETS: Uncomment one of the following lines\n",
    "    x_train, y_train, x_test, y_test = load_mnist_data(fashion=False)  # MNIST digits\n",
    "    # x_train, y_train, x_test, y_test = load_mnist_data(fashion=True)   # Fashion-MNIST\n",
    "    \n",
    "    # Normalize the data (z-score normalization)\n",
    "    m = x_train.mean(axis=0)\n",
    "    s = x_train.std(axis=0) + 1e-8\n",
    "    x_ntrain = (x_train - m) / s \n",
    "    x_ntest = (x_test - m) / s \n",
    "    \n",
    "    print(\"Running PCA component analysis...\")\n",
    "    print(\"Testing components from 10 to 780 (78 different values)\")\n",
    "    print()\n",
    "\n",
    "    n = 78\n",
    "    pcomp = np.linspace(10, 780, n, dtype=\"int16\")\n",
    "    nb = np.zeros((n, 4))  # [n_components, score, train_time, test_time]\n",
    "    rf = np.zeros((n, 4))\n",
    "    sv = np.zeros((n, 4))\n",
    "    tv = np.zeros((n, 2))  # [n_components, explained_variance_ratio]\n",
    "\n",
    "    print(\"Progress:\")\n",
    "    for i, p in enumerate(pcomp):\n",
    "        print(f\"Testing {p} components... ({i+1}/{n})\", end=\" \")\n",
    "        \n",
    "        pca = decomposition.PCA(n_components=p)\n",
    "        pca.fit(x_ntrain)\n",
    "        xtrain = pca.transform(x_ntrain)\n",
    "        xtest = pca.transform(x_ntest)\n",
    "        tv[i, :] = [p, pca.explained_variance_ratio_.sum()]\n",
    "        \n",
    "        # Test Naive Bayes\n",
    "        sc, etrn, etst = run(xtrain, y_train, xtest, y_test, GaussianNB())\n",
    "        nb[i, :] = [p, sc, etrn, etst]\n",
    "        \n",
    "        # Test Random Forest\n",
    "        sc, etrn, etst = run(xtrain, y_train, xtest, y_test, RandomForestClassifier(n_estimators=50))\n",
    "        rf[i, :] = [p, sc, etrn, etst]\n",
    "        \n",
    "        # Test Linear SVM\n",
    "        sc, etrn, etst = run(xtrain, y_train, xtest, y_test, LinearSVC(C=1.0, max_iter=2000))\n",
    "        sv[i, :] = [p, sc, etrn, etst]\n",
    "        \n",
    "        print(f\"Done. (Variance explained: {tv[i, 1]:.3f})\")\n",
    "\n",
    "    # Save results to local files\n",
    "    print(\"\\nSaving results...\")\n",
    "    np.save(\"mnist_pca_tv.npy\", tv)  # Total variance explained\n",
    "    np.save(\"mnist_pca_nb.npy\", nb)  # Naive Bayes results\n",
    "    np.save(\"mnist_pca_rf.npy\", rf)  # Random Forest results\n",
    "    np.save(\"mnist_pca_sv.npy\", sv)  # Linear SVM results\n",
    "    \n",
    "    print(\"Results saved to:\")\n",
    "    print(\"  - mnist_pca_tv.npy: [n_components, explained_variance_ratio]\")\n",
    "    print(\"  - mnist_pca_nb.npy: [n_components, score, train_time, test_time] for Naive Bayes\")\n",
    "    print(\"  - mnist_pca_rf.npy: [n_components, score, train_time, test_time] for Random Forest\")\n",
    "    print(\"  - mnist_pca_sv.npy: [n_components, score, train_time, test_time] for Linear SVM\")\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nSummary - Best performance by number of components:\")\n",
    "    print(\"Naive Bayes:\")\n",
    "    best_nb_idx = np.argmax(nb[:, 1])\n",
    "    print(f\"  Best: {nb[best_nb_idx, 0]:.0f} components, score: {nb[best_nb_idx, 1]:.4f}\")\n",
    "    \n",
    "    print(\"Random Forest:\")\n",
    "    best_rf_idx = np.argmax(rf[:, 1])\n",
    "    print(f\"  Best: {rf[best_rf_idx, 0]:.0f} components, score: {rf[best_rf_idx, 1]:.4f}\")\n",
    "    \n",
    "    print(\"Linear SVM:\")\n",
    "    best_sv_idx = np.argmax(sv[:, 1])\n",
    "    print(f\"  Best: {sv[best_sv_idx, 0]:.0f} components, score: {sv[best_sv_idx, 1]:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eb0b1c",
   "metadata": {},
   "source": [
    "## Classical Model Summary\n",
    "\n",
    "The chapter concludes with a summary of pros and cons for the six classical models discussed:\n",
    "\n",
    "### Nearest Centroid\n",
    "\n",
    "- **Pros:** Simple implementation, fast training, low memory use, supports multiclass classification, fast inference.\n",
    "- **Cons:** Assumes each class forms a tight cluster in feature space, often too simplistic for complex data. Variants with multiple centroids per class can improve performance.\n",
    "\n",
    "### k-Nearest Neighbors (k-NN)\n",
    "\n",
    "- **Pros:** No explicit training required, works well with large datasets, supports multiclass classification naturally.\n",
    "- **Cons:** Slow inference because distances must be computed for every training sample, even with optimized algorithms.\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "- **Pros:** Fast to train and classify, supports multiclass problems, works for both discrete and continuous features.\n",
    "- **Cons:** Assumes feature independence, which is rarely true in practice. Continuous features often require additional distributional assumptions (e.g., Gaussian).\n",
    "\n",
    "### Decision Trees\n",
    "\n",
    "- **Pros:** Fast training and inference, interpretable, supports multiclass and mixed feature types, can justify decisions with a clear path from root to leaf.\n",
    "- **Cons:** Prone to overfitting, interpretability decreases with tree size, requires balancing tree depth against accuracy.\n",
    "\n",
    "### Random Forests\n",
    "\n",
    "- **Pros:** Robust to overfitting, supports multiclass problems, reasonably fast to train and infer, less sensitive to feature scaling, accuracy improves with more trees.\n",
    "- **Cons:** Harder to interpret than single decision trees, inference time scales linearly with the number of trees, stochastic performance can vary slightly between trainings.\n",
    "\n",
    "### Support Vector Machines (SVMs)\n",
    "\n",
    "- **Pros:** Can achieve excellent performance, fast inference after training.\n",
    "- **Cons:** Multiclass requires multiple models, only supports continuous features, sensitive to feature scaling, difficult to train on large datasets with non-linear kernels, requires careful hyperparameter tuning.\n",
    "\n",
    "## When to Use Classical Models\n",
    "\n",
    "Classical models remain appropriate under certain conditions:\n",
    "\n",
    "1. **Small datasets:** They perform well when there are only tens or hundreds of examples, unlike deep learning models that require larger datasets.\n",
    "2. **Limited computational resources:** Simple models (nearest centroid, naive Bayes, decision trees, SVMs) are feasible on low-power devices; k-NN may be too slow unless the dataset is small.\n",
    "3. **Explainability:** Models like decision trees, k-NN, nearest centroid, and naive Bayes can explain their predictions, unlike deep neural networks.\n",
    "4. **Vector inputs without structure:** When features are independent and unstructured (not spatially correlated as in images), classical models are suitable.\n",
    "\n",
    "These are rules of thumb, not hard rules. Deep learning could be used even when these conditions apply, but classic models may provide sufficient performance with less complexity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de21f07c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
